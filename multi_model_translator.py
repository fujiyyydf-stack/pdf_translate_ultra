#!/usr/bin/env python3
"""
å¤šæ¨¡å‹ç¿»è¯‘æ•´åˆå™¨ - è°ƒç”¨å¤šä¸ªæ¨¡å‹ç¿»è¯‘ï¼Œç„¶åæ•´åˆå‡ºæœ€ä¼˜ç»“æœ
"""

import os
import json
import time
import threading
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

from openai import OpenAI
from dotenv import load_dotenv
from tqdm import tqdm

load_dotenv()


class MultiModelTranslator:
    """å¤šæ¨¡å‹ç¿»è¯‘æ•´åˆå™¨"""
    
    # é»˜è®¤çš„ç¿»è¯‘æ¨¡å‹åˆ—è¡¨ï¼ˆä½¿ç”¨ä¸åŒæ¨¡å‹ä»¥è·å¾—å¤šæ ·åŒ–ç¿»è¯‘ï¼‰
    DEFAULT_TRANSLATION_MODELS = [
        "x-ai/grok-4.1-fast",
        "x-ai/grok-4.1-fast", 
        "x-ai/grok-4.1-fast",
    ]
    
    # é»˜è®¤çš„æ•´åˆæ¨¡å‹
    DEFAULT_INTEGRATION_MODEL = "x-ai/grok-4.1-fast"
    
    # é»˜è®¤ç¿»è¯‘æç¤ºè¯
    DEFAULT_TRANSLATION_PROMPT = """ä½ æ˜¯ä¸€ä½ç²¾é€šæ³•è¯­å’Œä¸­æ–‡çš„ä¸“ä¸šç¿»è¯‘å®˜ã€‚
ä½ çš„ä»»åŠ¡æ˜¯å°†è¾“å…¥çš„æ³•è¯­æ–‡æœ¬ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚
è¦æ±‚ï¼š
1. ä¿æŒç¿»è¯‘çš„å­¦æœ¯æ€§æˆ–æ–‡å­¦æ€§ï¼Œè¯­æ°”çµåŠ¨è‡ªç„¶ä¼˜é›…ï¼Œè¡¨è¾¾é¡ºç•…å‡†ç¡®ï¼Œç¡®ä¿è¯­å¥ç»“æ„å®Œæ•´ï¼Œä¼˜å…ˆä½¿ç”¨ç®€æ´å¥å¼ï¼Œä¿è¯è¡Œæ–‡æ˜“æ‡‚ï¼Œä¿è¯æ‰€æœ‰å“²å­¦å­¦æœ¯æœ¯è¯­çš„å‡†ç¡®æ€§ï¼Œåœ¨ä½ è§‰å¾—é‡è¦çš„å“²å­¦å­¦æœ¯æœ¯è¯­åé¢åŠ æ‹¬å·ï¼Œåœ¨æ‹¬å·å†…æ ‡æ³¨åŸæ–‡å¹¶è¿›è¡Œä¸€åˆ°ä¸¤å¥è¯çš„è§£é‡Šç”¨äºè¯¥æ–‡æœ¬çš„è„šæ³¨ã€‚
2. ä¿ç•™åŸæœ‰çš„æ ¼å¼ï¼ˆå¦‚æ ‡é¢˜ã€åˆ—è¡¨ï¼‰ã€‚
3. ç›´æ¥è¿”å›ç¿»è¯‘åçš„ä¸­æ–‡å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–è¯´æ˜ã€‚
"""
    
    # é»˜è®¤æ•´åˆæç¤ºè¯
    DEFAULT_INTEGRATION_PROMPT = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç¿»è¯‘å®¡æ ¡ä¸ç¼–è¾‘ä¸“å®¶ï¼Œç²¾é€šæ³•è¯­å’Œä¸­æ–‡ï¼Œå°¤å…¶æ“…é•¿å“²å­¦æ–‡æœ¬çš„ç¿»è¯‘ä¸ç¼–è¾‘ã€‚

ä½ å°†æ”¶åˆ°ï¼š
1. ä¸€æ®µæ³•è¯­åŸæ–‡ï¼ˆæ³¨æ„ï¼šåŸæ–‡æ¥è‡ªPDFæå–ï¼Œå¯èƒ½å­˜åœ¨æ ¼å¼é—®é¢˜ï¼‰
2. å¤šä¸ªä¸­æ–‡ç¿»è¯‘ç‰ˆæœ¬

ä½ çš„ä»»åŠ¡æ˜¯**æ•´åˆå‡ºæœ€ä¼˜ç¿»è¯‘**ï¼ŒåŒæ—¶æ‰¿æ‹…**ç¼–è¾‘æ¢³ç†**å·¥ä½œã€‚

**è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼éµå®ˆï¼‰**ï¼š
```
[åˆ†æ]
ç®€è¦è¯´æ˜ï¼šé‡‡ç”¨äº†å“ªä¸ªç‰ˆæœ¬çš„ä¼˜ç‚¹ï¼Œä¿®æ­£äº†ä»€ä¹ˆé—®é¢˜ï¼Œæ¸…ç†äº†ä»€ä¹ˆæ ¼å¼é”™è¯¯

[è¯‘æ–‡]
æœ€ç»ˆæ•´åˆçš„ç¿»è¯‘å†…å®¹
```

**æ•´åˆåŸåˆ™**ï¼š
- å–å„ç‰ˆæœ¬ä¹‹é•¿ï¼Œé¿å…¶æ‰€çŸ­
- ä¿æŒæœ¯è¯­ç¿»è¯‘å‡†ç¡®ä¸€è‡´
- è„šæ³¨ç²¾ç‚¼ï¼Œä¸è¦è¿‡å¤š

**ç¼–è¾‘æ¢³ç†ï¼ˆé‡è¦ï¼‰**ï¼š
- æ£€æŸ¥å¹¶åˆ é™¤æ®‹ç•™çš„æ°´å°ã€é¡µç ã€æ–‡ä»¶åç­‰æ— å…³å†…å®¹ï¼ˆå¦‚æ—¥æœŸæ—¶é—´æˆ³ã€"Ã‰PREUVES"ã€".indd"ç­‰ï¼‰
- ä¿®æ­£PDFæå–å¯¼è‡´çš„æ ¼å¼é—®é¢˜ï¼ˆå¦‚æ–­è¡Œé”™è¯¯ã€å¤šä½™ç©ºæ ¼ã€ä¹±ç å­—ç¬¦ï¼‰
- ç¡®ä¿æ®µè½ç»“æ„æ¸…æ™°ã€è¡Œæ–‡æµç•…
- å¦‚æœåŸæ–‡æœ‰æ˜æ˜¾çš„OCRé”™è¯¯æˆ–ä¹±ç ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡åˆç†ä¿®æ­£
"""

    def __init__(
        self,
        api_key: str = None,
        base_url: str = None,
        translation_models: list = None,
        integration_model: str = None,
        output_dir: str = "output",
        system_prompt: str = None,
        integration_prompt: str = None,
        model_prompts: list = None
    ):
        """
        åˆå§‹åŒ–å¤šæ¨¡å‹ç¿»è¯‘å™¨
        
        Args:
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URLï¼ˆå¦‚OpenRouterï¼‰
            translation_models: ç”¨äºç¿»è¯‘çš„æ¨¡å‹åˆ—è¡¨ï¼ˆé»˜è®¤3ä¸ªï¼‰
            integration_model: ç”¨äºæ•´åˆçš„æ¨¡å‹
            output_dir: è¾“å‡ºç›®å½•
            system_prompt: é»˜è®¤ç¿»è¯‘æç¤ºè¯ï¼ˆå½“model_promptsæœªæŒ‡å®šæ—¶ä½¿ç”¨ï¼‰
            integration_prompt: è‡ªå®šä¹‰æ•´åˆæç¤ºè¯
            model_prompts: æ¯ä¸ªæ¨¡å‹çš„ç‹¬ç«‹æç¤ºè¯åˆ—è¡¨ï¼ˆä¸translation_modelsä¸€ä¸€å¯¹åº”ï¼‰
        """
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.base_url = base_url or os.getenv("OPENAI_BASE_URL")
        self.translation_models = translation_models or self.DEFAULT_TRANSLATION_MODELS
        self.integration_model = integration_model or self.DEFAULT_INTEGRATION_MODEL
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        client_kwargs = {"api_key": self.api_key}
        if self.base_url:
            client_kwargs["base_url"] = self.base_url
        self.client = OpenAI(**client_kwargs)
        
        # é»˜è®¤ç¿»è¯‘æç¤ºè¯
        self.translation_prompt = system_prompt or self.DEFAULT_TRANSLATION_PROMPT
        
        # æ¯ä¸ªæ¨¡å‹çš„ç‹¬ç«‹æç¤ºè¯ï¼ˆå¦‚æœæä¾›ï¼‰
        # å¦‚æœmodel_promptsé•¿åº¦ä¸å¤Ÿï¼Œç”¨é»˜è®¤æç¤ºè¯è¡¥é½
        self.model_prompts = []
        if model_prompts:
            for i, model in enumerate(self.translation_models):
                if i < len(model_prompts) and model_prompts[i]:
                    self.model_prompts.append(model_prompts[i])
                else:
                    self.model_prompts.append(self.translation_prompt)
        else:
            self.model_prompts = [self.translation_prompt] * len(self.translation_models)
        
        # æ•´åˆæç¤ºè¯ï¼ˆæ”¯æŒè‡ªå®šä¹‰ï¼‰
        self.integration_prompt = integration_prompt or self.DEFAULT_INTEGRATION_PROMPT

    def _call_model(self, model: str, system_prompt: str, user_content: str, 
                    retry_count: int = 3) -> str:
        """
        è°ƒç”¨æŒ‡å®šæ¨¡å‹
        
        Args:
            model: æ¨¡å‹åç§°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_content: ç”¨æˆ·å†…å®¹
            retry_count: é‡è¯•æ¬¡æ•°
            
        Returns:
            æ¨¡å‹è¿”å›çš„å†…å®¹
        """
        for attempt in range(retry_count):
            try:
                response = self.client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_content}
                    ],
                    temperature=0.3,
                    max_tokens=8000
                )
                result = response.choices[0].message.content
                if result and result.strip():
                    return result.strip()
                else:
                    if attempt < retry_count - 1:
                        time.sleep(2 ** attempt)
                        continue
            except Exception as e:
                if attempt < retry_count - 1:
                    time.sleep(2 ** attempt)
                else:
                    return f"[æ¨¡å‹ {model} è°ƒç”¨å¤±è´¥: {str(e)}]"
        
        return f"[æ¨¡å‹ {model} è¿”å›ä¸ºç©º]"

    def translate_with_single_model(self, text: str, model: str, prompt: str = None) -> str:
        """ä½¿ç”¨å•ä¸ªæ¨¡å‹ç¿»è¯‘"""
        user_content = f"è¯·å°†ä»¥ä¸‹æ³•è¯­æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ï¼š\n\n{text}"
        system_prompt = prompt or self.translation_prompt
        return self._call_model(model, system_prompt, user_content)

    def translate_segment_multi(self, original_text: str) -> dict:
        """
        ä½¿ç”¨å¤šä¸ªæ¨¡å‹ç¿»è¯‘åŒä¸€æ®µæ–‡æœ¬ï¼Œæ¯ä¸ªæ¨¡å‹ä½¿ç”¨å…¶ç‹¬ç«‹çš„æç¤ºè¯
        
        Args:
            original_text: åŸæ–‡
            
        Returns:
            åŒ…å«å„æ¨¡å‹ç¿»è¯‘ç»“æœçš„å­—å…¸ï¼Œkeyä¸º "æ¨¡å‹å_åºå·" ä»¥é¿å…é‡å¤
        """
        translations = {}
        
        # å¹¶å‘è°ƒç”¨å¤šä¸ªç¿»è¯‘æ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡å‹ä½¿ç”¨å…¶ç‹¬ç«‹çš„æç¤ºè¯
        with ThreadPoolExecutor(max_workers=len(self.translation_models)) as executor:
            # ä½¿ç”¨enumerateä¸ºæ¯ä¸ªæ¨¡å‹æ·»åŠ åºå·ï¼Œé¿å…ç›¸åŒæ¨¡å‹åè¦†ç›–
            futures = {
                executor.submit(
                    self.translate_with_single_model, 
                    original_text, 
                    model, 
                    self.model_prompts[idx]  # ä½¿ç”¨è¯¥æ¨¡å‹çš„ç‹¬ç«‹æç¤ºè¯
                ): (idx, model)
                for idx, model in enumerate(self.translation_models)
            }
            
            for future in as_completed(futures):
                idx, model = futures[future]
                # ä½¿ç”¨ "åºå·_æ¨¡å‹å" ä½œä¸ºkeyï¼Œç¡®ä¿å”¯ä¸€æ€§
                key = f"{idx+1}_{model}"
                try:
                    translations[key] = future.result()
                except Exception as e:
                    translations[key] = f"[ç¿»è¯‘å¤±è´¥: {str(e)}]"
        
        return translations

    def integrate_translations(self, original_text: str, translations: dict) -> dict:
        """
        æ•´åˆå¤šä¸ªç¿»è¯‘ç‰ˆæœ¬ï¼Œç”Ÿæˆæœ€ä¼˜ç»“æœ
        
        Args:
            original_text: æ³•è¯­åŸæ–‡
            translations: å„æ¨¡å‹çš„ç¿»è¯‘ç»“æœ {model: translation}
            
        Returns:
            åŒ…å«reasoningå’Œè¯‘æ–‡çš„å­—å…¸
        """
        # æ„å»ºæ•´åˆè¯·æ±‚
        user_content = f"""## æ³•è¯­åŸæ–‡

{original_text}

## ç¿»è¯‘ç‰ˆæœ¬

"""
        for i, (model, trans) in enumerate(sorted(translations.items()), 1):
            # ä»keyä¸­æå–æ¨¡å‹åï¼ˆå»æ‰åºå·å‰ç¼€ï¼‰
            model_display = model.split("_", 1)[-1] if "_" in model else model
            model_short = model_display.split("/")[-1] if "/" in model_display else model_display
            user_content += f"### è¯‘è€…{i} ({model_short})\n\n{trans}\n\n"
        
        user_content += "## è¯·æŒ‰æ ¼å¼è¾“å‡ºåˆ†æå’Œæ•´åˆè¯‘æ–‡"
        
        raw_result = self._call_model(
            self.integration_model, 
            self.integration_prompt, 
            user_content
        )
        
        # è§£æè¿”å›ç»“æœï¼Œæå–reasoningå’Œè¯‘æ–‡
        reasoning = ""
        integrated = raw_result
        
        if "[åˆ†æ]" in raw_result and "[è¯‘æ–‡]" in raw_result:
            parts = raw_result.split("[è¯‘æ–‡]")
            if len(parts) == 2:
                reasoning_part = parts[0]
                integrated = parts[1].strip()
                # æå–åˆ†æå†…å®¹
                if "[åˆ†æ]" in reasoning_part:
                    reasoning = reasoning_part.split("[åˆ†æ]")[-1].strip()
        elif "åˆ†æ" in raw_result[:50] or "è¯‘æ–‡" in raw_result[:100]:
            # å°è¯•å…¶ä»–æ ¼å¼
            lines = raw_result.split("\n")
            in_reasoning = False
            in_translation = False
            reasoning_lines = []
            translation_lines = []
            
            for line in lines:
                if "åˆ†æ" in line and len(line) < 20:
                    in_reasoning = True
                    in_translation = False
                    continue
                elif "è¯‘æ–‡" in line and len(line) < 20:
                    in_reasoning = False
                    in_translation = True
                    continue
                
                if in_reasoning:
                    reasoning_lines.append(line)
                elif in_translation:
                    translation_lines.append(line)
            
            if reasoning_lines:
                reasoning = "\n".join(reasoning_lines).strip()
            if translation_lines:
                integrated = "\n".join(translation_lines).strip()
        
        return {
            "reasoning": reasoning,
            "text": integrated if integrated else raw_result
        }

    def translate_segment_with_integration(self, original_text: str) -> dict:
        """
        å®Œæ•´çš„å¤šæ¨¡å‹ç¿»è¯‘+æ•´åˆæµç¨‹
        
        Args:
            original_text: åŸæ–‡
            
        Returns:
            åŒ…å«å„æ¨¡å‹ç¿»è¯‘å’Œæœ€ç»ˆæ•´åˆç»“æœçš„å­—å…¸
        """
        # ç¬¬ä¸€æ­¥ï¼šå¤šæ¨¡å‹ç¿»è¯‘
        translations = self.translate_segment_multi(original_text)
        
        # ç¬¬äºŒæ­¥ï¼šæ•´åˆï¼ˆè¿”å›åŒ…å«reasoningçš„ç»“æœï¼‰
        integration_result = self.integrate_translations(original_text, translations)
        
        return {
            "individual_translations": translations,
            "reasoning": integration_result["reasoning"],
            "integrated": integration_result["text"]
        }


class MultiModelPDFTranslator:
    """å¤šæ¨¡å‹PDFç¿»è¯‘å™¨ - æ•´åˆåˆ°PDFç¿»è¯‘æµç¨‹"""
    
    def __init__(
        self,
        api_key: str = None,
        base_url: str = None,
        translation_models: list = None,
        integration_model: str = None,
        max_chars_per_segment: int = 2000,
        output_dir: str = "output",
        header_ratio: float = 0.08,
        footer_ratio: float = 0.92
    ):
        """
        åˆå§‹åŒ–
        
        Args:
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            translation_models: ç¿»è¯‘æ¨¡å‹åˆ—è¡¨
            integration_model: æ•´åˆæ¨¡å‹
            max_chars_per_segment: æ¯æ®µæœ€å¤§å­—ç¬¦æ•°
            output_dir: è¾“å‡ºç›®å½•
            header_ratio: é¡µçœ‰åŒºåŸŸæ¯”ä¾‹
            footer_ratio: é¡µè„šåŒºåŸŸæ¯”ä¾‹
        """
        self.multi_translator = MultiModelTranslator(
            api_key=api_key,
            base_url=base_url,
            translation_models=translation_models,
            integration_model=integration_model,
            output_dir=output_dir
        )
        
        self.max_chars = max_chars_per_segment
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.header_ratio = header_ratio
        self.footer_ratio = footer_ratio
        
        # å¯¼å…¥åŸæœ‰çš„PDFå¤„ç†åŠŸèƒ½
        from pdf_translator import PDFTranslator, DEFAULT_FILTER_PATTERNS
        self._pdf_helper = PDFTranslator(
            api_key=api_key or "dummy",  # åªç”¨äºPDFå¤„ç†ï¼Œä¸ç”¨äºç¿»è¯‘
            base_url=base_url,
            max_chars_per_segment=max_chars_per_segment,
            output_dir=output_dir,
            header_ratio=header_ratio,
            footer_ratio=footer_ratio
        )

    def load_progress(self, progress_file: Path) -> dict:
        """åŠ è½½è¿›åº¦"""
        if progress_file.exists():
            try:
                with open(progress_file, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    if content:
                        return json.loads(content)
            except:
                pass
        return {"completed": [], "translations": {}}

    def save_progress(self, progress_file: Path, progress: dict):
        """ä¿å­˜è¿›åº¦"""
        progress_to_save = {
            "completed": sorted(progress["completed"]),
            "translations": progress["translations"]
        }
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_to_save, f, ensure_ascii=False, indent=2)

    def translate_pdf(
        self,
        pdf_path: str,
        start_page: int = None,
        end_page: int = None,
        max_workers: int = 10
    ) -> str:
        """
        ä½¿ç”¨å¤šæ¨¡å‹ç¿»è¯‘PDF
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            start_page: èµ·å§‹é¡µç 
            end_page: ç»“æŸé¡µç 
            max_workers: å¹¶å‘å¤„ç†çš„æ®µè½æ•°
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        pdf_path = Path(pdf_path)
        pdf_name = pdf_path.stem
        
        # ç”Ÿæˆæ¨¡å‹æ ‡è¯†ï¼ˆé€‚é…ç›¸åŒæ¨¡å‹å’Œä¸åŒæ¨¡å‹çš„æƒ…å†µï¼‰
        trans_models = self.multi_translator.translation_models
        unique_models = list(set(trans_models))
        
        if len(unique_models) == 1:
            # ç›¸åŒæ¨¡å‹å¤šæ¬¡è°ƒç”¨ï¼šmulti3x_grok-4-1-fast
            model_short = unique_models[0].split("/")[-1].replace(".", "-")[:12]
            trans_suffix = f"multi{len(trans_models)}x_{model_short}"
        else:
            # ä¸åŒæ¨¡å‹ï¼šmulti_claude+gpt4+deepseek
            model_shorts = [m.split("/")[-1].replace(".", "-")[:8] for m in unique_models]
            trans_suffix = f"multi_{'+'.join(model_shorts)}"
        
        integration_short = self.multi_translator.integration_model.split("/")[-1].replace(".", "-")[:12]
        model_suffix = f"{trans_suffix}_int_{integration_short}"
        
        progress_file = self.output_dir / f"{pdf_name}_progress_{model_suffix}.json"
        
        print(f"ğŸ“– æ­£åœ¨è¯»å–PDF: {pdf_path}")
        print(f"ğŸ¤– ç¿»è¯‘æ¨¡å‹: {', '.join(self.multi_translator.translation_models)}")
        print(f"ğŸ”§ æ•´åˆæ¨¡å‹: {self.multi_translator.integration_model}")
        
        # ä½¿ç”¨åŸæœ‰çš„PDFå¤„ç†åŠŸèƒ½
        pages_text = self._pdf_helper.extract_text_from_pdf(str(pdf_path))
        print(f"âœ… æˆåŠŸæå– {len(pages_text)} é¡µæ–‡æœ¬")
        
        # ç­›é€‰é¡µé¢èŒƒå›´
        if start_page or end_page:
            pages_text = [
                p for p in pages_text 
                if (start_page is None or p["page"] >= start_page) and
                   (end_page is None or p["page"] <= end_page)
            ]
            print(f"ğŸ“„ é€‰æ‹©é¡µé¢èŒƒå›´: {start_page or 1} - {end_page or 'æœ«é¡µ'}")
        
        # åˆ†æ®µ
        segments = self._pdf_helper.split_into_segments(pages_text)
        print(f"ğŸ“ å…±åˆ†å‰²æˆ {len(segments)} ä¸ªç¿»è¯‘æ®µè½")
        
        # åŠ è½½è¿›åº¦
        progress = self.load_progress(progress_file)
        completed_ids = set(progress["completed"])
        translations = progress["translations"]
        
        if completed_ids:
            print(f"ğŸ“Œ å‘ç°å·²æœ‰è¿›åº¦ï¼Œå·²å®Œæˆ {len(completed_ids)}/{len(segments)} æ®µ")
        
        # ç­›é€‰éœ€è¦ç¿»è¯‘çš„æ®µè½
        segments_to_translate = []
        for segment in segments:
            seg_id = segment["id"]
            existing = translations.get(str(seg_id), {})
            existing_trans = existing.get("integrated", "")
            
            if str(seg_id) in completed_ids or seg_id in completed_ids:
                if existing_trans and existing_trans.strip():
                    continue
            
            segments_to_translate.append(segment)
        
        if not segments_to_translate:
            print("âœ… æ‰€æœ‰æ®µè½å·²ç¿»è¯‘å®Œæˆï¼")
        else:
            print(f"\nğŸš€ å¼€å§‹å¤šæ¨¡å‹ç¿»è¯‘... (å¹¶å‘æ®µè½æ•°: {max_workers})")
            print("=" * 50)
            
            lock = threading.Lock()
            
            def process_segment(segment):
                seg_id = segment["id"]
                try:
                    result = self.multi_translator.translate_segment_with_integration(segment["text"])
                    
                    with lock:
                        translations[str(seg_id)] = {
                            "page": segment["page"],
                            "original": segment["text"],
                            "individual": result["individual_translations"],
                            "reasoning": result["reasoning"],  # æ–°å¢ï¼šæ•´åˆåˆ†æ
                            "integrated": result["integrated"]
                        }
                        
                        if seg_id not in progress["completed"]:
                            progress["completed"].append(seg_id)
                        
                        self.save_progress(progress_file, progress)
                    
                    return {"id": seg_id, "success": True}
                except Exception as e:
                    return {"id": seg_id, "success": False, "error": str(e)}
            
            try:
                with tqdm(total=len(segments_to_translate), desc="å¤šæ¨¡å‹ç¿»è¯‘") as pbar:
                    with ThreadPoolExecutor(max_workers=max_workers) as executor:
                        futures = {
                            executor.submit(process_segment, seg): seg
                            for seg in segments_to_translate
                        }
                        
                        for future in as_completed(futures):
                            result = future.result()
                            pbar.update(1)
                            if not result["success"]:
                                print(f"\nâš ï¸  æ®µè½ {result['id']} å¤±è´¥: {result.get('error')}")
                                
            except KeyboardInterrupt:
                print("\n\nâ¸ï¸  ç¿»è¯‘å·²æš‚åœï¼Œè¿›åº¦å·²ä¿å­˜ã€‚")
                return None
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
        print("\nğŸ“„ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£...")
        
        # æ•´åˆç‰ˆè¯‘æ–‡
        output_file = self.output_dir / f"{pdf_name}_translated_{model_suffix}.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("â”" + "â”" * 58 + "â”“\n")
            f.write("â”ƒ" + " å¤šæ¨¡å‹æ•´åˆç¿»è¯‘ ".center(54) + "â”ƒ\n")
            f.write("â”£" + "â”" * 58 + "â”«\n")
            f.write("â”ƒ" + f" ç¿»è¯‘: {', '.join([m.split('/')[-1] for m in self.multi_translator.translation_models])} ".center(54) + "â”ƒ\n")
            f.write("â”ƒ" + f" æ•´åˆ: {self.multi_translator.integration_model.split('/')[-1]} ".center(54) + "â”ƒ\n")
            f.write("â”—" + "â”" * 58 + "â”›\n\n")
            
            current_page = None
            for i in range(len(segments)):
                trans_data = translations.get(str(i), {})
                page = trans_data.get("page", segments[i]["page"])
                integrated = trans_data.get("integrated", "[æœªç¿»è¯‘]")
                
                if page != current_page:
                    f.write("\n")
                    f.write("â•”" + "â•" * 20 + f" ç¬¬ {page} é¡µ " + "â•" * 20 + "â•—\n")
                    f.write("\n")
                    current_page = page
                
                f.write(integrated + "\n\n")
        
        # åŒè¯­å¯¹ç…§ç‰ˆï¼ˆåŒ…å«æ‰€æœ‰ç¿»è¯‘ç‰ˆæœ¬ï¼‰
        bilingual_file = self.output_dir / f"{pdf_name}_bilingual_{model_suffix}.txt"
        with open(bilingual_file, 'w', encoding='utf-8') as f:
            f.write("â”" + "â”" * 58 + "â”“\n")
            f.write("â”ƒ" + " å¤šæ¨¡å‹ç¿»è¯‘å¯¹ç…§ ".center(54) + "â”ƒ\n")
            f.write("â”—" + "â”" * 58 + "â”›\n\n")
            
            current_page = None
            for i in range(len(segments)):
                trans_data = translations.get(str(i), {})
                page = trans_data.get("page", segments[i]["page"])
                original = trans_data.get("original", segments[i]["text"])
                individual = trans_data.get("individual", {})
                reasoning = trans_data.get("reasoning", "")
                integrated = trans_data.get("integrated", "[æœªç¿»è¯‘]")
                
                if page != current_page:
                    f.write("\n")
                    f.write("â•”" + "â•" * 20 + f" ç¬¬ {page} é¡µ " + "â•" * 20 + "â•—\n")
                    f.write("\n")
                    current_page = page
                
                f.write("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n")
                f.write("â”‚ ã€åŸæ–‡ã€‘                                â”‚\n")
                f.write("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n")
                f.write(original + "\n\n")
                
                # å„æ¨¡å‹ç¿»è¯‘ï¼ˆæŒ‰åºå·æ’åºï¼‰
                for model, trans in sorted(individual.items()):
                    # ä»keyä¸­æå–æ˜¾ç¤ºåï¼ˆå»æ‰åºå·å‰ç¼€å¦‚ "1_"ï¼‰
                    model_display = model.split("_", 1)[-1] if "_" in model else model
                    model_short = model_display.split("/")[-1] if "/" in model_display else model_display
                    # æå–åºå·
                    idx = model.split("_")[0] if "_" in model else ""
                    f.write(f"â”Œâ”€â”€â”€ ã€è¯‘è€…{idx}: {model_short}ã€‘ â”€â”€â”€â”\n")
                    f.write(trans + "\n\n")
                
                # æ•´åˆåˆ†æï¼ˆå¦‚æœæœ‰ï¼‰
                if reasoning:
                    f.write("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n")
                    f.write("â”‚ ã€ğŸ” æ•´åˆåˆ†æã€‘                         â”‚\n")
                    f.write("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n")
                    f.write(reasoning + "\n\n")
                
                f.write("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n")
                f.write("â”‚ ã€âœ¨ æ•´åˆè¯‘æ–‡ã€‘                         â”‚\n")
                f.write("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n")
                f.write(integrated + "\n")
                f.write("â•" * 50 + "\n\n")
        
        print(f"\nâœ¨ ç¿»è¯‘å®Œæˆ!")
        print(f"ğŸ“ æ•´åˆè¯‘æ–‡: {output_file}")
        print(f"ğŸ“ å¤šç‰ˆæœ¬å¯¹ç…§: {bilingual_file}")
        
        return str(output_file)


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="å¤šæ¨¡å‹ç¿»è¯‘æ•´åˆå™¨ - è°ƒç”¨å¤šä¸ªæ¨¡å‹ç¿»è¯‘åæ•´åˆæœ€ä¼˜ç»“æœ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä½¿ç”¨é»˜è®¤æ¨¡å‹ç¿»è¯‘
  python multi_model_translator.py book.pdf
  
  # æŒ‡å®šç¿»è¯‘æ¨¡å‹
  python multi_model_translator.py book.pdf \\
    --models "anthropic/claude-3.5-sonnet,openai/gpt-4o,deepseek/deepseek-chat"
  
  # æŒ‡å®šæ•´åˆæ¨¡å‹
  python multi_model_translator.py book.pdf --integration-model "anthropic/claude-3.5-sonnet"
  
  # ç¿»è¯‘æŒ‡å®šé¡µé¢
  python multi_model_translator.py book.pdf --start 1 --end 10

é»˜è®¤é…ç½®:
  ç¿»è¯‘æ¨¡å‹: claude-3.5-sonnet, gpt-4o, deepseek-chat
  æ•´åˆæ¨¡å‹: claude-3.5-sonnet
        """
    )
    
    parser.add_argument("pdf_path", help="PDFæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--start", type=int, help="èµ·å§‹é¡µç ")
    parser.add_argument("--end", type=int, help="ç»“æŸé¡µç ")
    parser.add_argument("--models", help="ç¿»è¯‘æ¨¡å‹åˆ—è¡¨ï¼Œé€—å·åˆ†éš”")
    parser.add_argument("--integration-model", help="æ•´åˆæ¨¡å‹")
    parser.add_argument("--workers", type=int, default=10, help="å¹¶å‘æ®µè½æ•° (é»˜è®¤: 10)")
    parser.add_argument("--output", default="output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--api-key", help="APIå¯†é’¥")
    parser.add_argument("--base-url", help="APIåŸºç¡€URL")
    
    args = parser.parse_args()
    
    if not Path(args.pdf_path).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.pdf_path}")
        return 1
    
    api_key = args.api_key or os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ æœªè®¾ç½®APIå¯†é’¥")
        return 1
    
    # è§£ææ¨¡å‹åˆ—è¡¨
    translation_models = None
    if args.models:
        translation_models = [m.strip() for m in args.models.split(",")]
    
    translator = MultiModelPDFTranslator(
        api_key=api_key,
        base_url=args.base_url or os.getenv("OPENAI_BASE_URL"),
        translation_models=translation_models,
        integration_model=args.integration_model,
        output_dir=args.output
    )
    
    translator.translate_pdf(
        pdf_path=args.pdf_path,
        start_page=args.start,
        end_page=args.end,
        max_workers=args.workers
    )
    
    return 0


if __name__ == "__main__":
    exit(main())
