#!/usr/bin/env python3
"""
æ–‡æœ¬å¯¹é½æ¨¡å—
åŠŸèƒ½ï¼šå°† PDF åŸæ–‡æ®µè½ä¸ Word è¯‘æ–‡æ®µè½è¿›è¡Œå¯¹é½åŒ¹é…
æ”¯æŒï¼šåŸºäºè§„åˆ™çš„å¯¹é½ å’Œ åŸºäºå¤§æ¨¡å‹çš„æ™ºèƒ½å¯¹é½
"""

import re
import os
import json
from typing import List, Dict, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()


class TextAligner:
    """æ–‡æœ¬æ®µè½å¯¹é½å™¨"""
    
    # æ™ºèƒ½å¯¹é½æç¤ºè¯
    ALIGNMENT_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘æ–‡æœ¬å¯¹é½ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†ææ³•è¯­åŸæ–‡å’Œä¸­æ–‡è¯‘æ–‡ï¼Œæ‰¾å‡ºå®ƒä»¬çš„å¯¹åº”å…³ç³»ã€‚

## è¾“å…¥è¯´æ˜
- ä½ ä¼šæ”¶åˆ°ä¸€ç³»åˆ—ç¼–å·çš„æ³•è¯­åŸæ–‡æ®µè½
- ä½ ä¼šæ”¶åˆ°ä¸€ç³»åˆ—ç¼–å·çš„ä¸­æ–‡è¯‘æ–‡æ®µè½
- æ³¨æ„ï¼šè¿™åªæ˜¯æ–‡æ¡£çš„ä¸€éƒ¨åˆ†ï¼Œè¯‘æ–‡çª—å£å¯èƒ½ä¸å¤Ÿå®Œæ•´ï¼Œæœ‰äº›åŸæ–‡çš„è¯‘æ–‡å¯èƒ½åœ¨åç»­æ®µè½ä¸­
- åŸæ–‡å’Œè¯‘æ–‡å¯èƒ½ä¸æ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼Œå­˜åœ¨ä»¥ä¸‹å¤æ‚æƒ…å†µï¼š
  - ä¸€ä¸ªè¯‘æ–‡å¯¹åº”å¤šä¸ªåŸæ–‡ï¼ˆè¯‘è€…åˆå¹¶ç¿»è¯‘ï¼‰
  - ä¸€ä¸ªåŸæ–‡å¯¹åº”å¤šä¸ªè¯‘æ–‡ï¼ˆè¯‘è€…æ‹†åˆ†ç¿»è¯‘ï¼‰
  - å¤šä¸ªè¯‘æ–‡å¯èƒ½æœ‰é‡å ï¼ˆåŒä¸€åŸæ–‡è¢«å¤šä¸ªè¯‘æ–‡éƒ¨åˆ†è¦†ç›–ï¼‰
  - æœ‰çš„åŸæ–‡å¯èƒ½æ²¡æœ‰è¢«ç¿»è¯‘ï¼ˆæ¼è¯‘ï¼‰
  - æœ‰çš„è¯‘æ–‡å¯èƒ½æ˜¯è¯‘è€…æ·»åŠ çš„ï¼ˆåŸæ–‡æ²¡æœ‰ï¼‰

## ä»»åŠ¡
1. åˆ†ææ¯ä¸ªåŸæ–‡æ®µè½å¯¹åº”å“ªä¸ªï¼ˆæˆ–å“ªäº›ï¼‰è¯‘æ–‡æ®µè½
2. å¯¹äºæ²¡æœ‰æ‰¾åˆ°è¯‘æ–‡çš„åŸæ–‡ï¼Œåˆ¤æ–­æ˜¯"çœŸçš„æ¼è¯‘"è¿˜æ˜¯"è¯‘æ–‡å¯èƒ½åœ¨åé¢"
3. è¯†åˆ«æ¼è¯‘å’Œè¯‘è€…æ·»åŠ çš„å†…å®¹

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼éµå®ˆJSONæ ¼å¼ï¼‰
```json
{
  "source_to_translation": [
    {"source_id": 1, "translation_ids": [1], "status": "matched", "confidence": "high"},
    {"source_id": 2, "translation_ids": [2], "status": "matched", "confidence": "high"},
    {"source_id": 3, "translation_ids": [], "status": "not_found_maybe_later", "reason": "å†…å®¹çœ‹èµ·æ¥åº”è¯¥æœ‰è¯‘æ–‡ï¼Œå¯èƒ½åœ¨åé¢"},
    {"source_id": 4, "translation_ids": [], "status": "not_found_skip", "reason": "å‡ºç‰ˆä¿¡æ¯/é¡µçœ‰é¡µè„šï¼Œé€šå¸¸ä¸ç¿»è¯‘"},
    {"source_id": 5, "translation_ids": [], "status": "missing", "reason": "æ­£æ–‡å†…å®¹ä½†ç¡®å®æ²¡æ‰¾åˆ°è¯‘æ–‡ï¼Œå¯èƒ½æ¼è¯‘"}
  ],
  "unmatched_translations": [4, 5],
  "window_status": {
    "all_sources_covered": false,
    "need_expand_window": true,
    "uncovered_sources": [3],
    "suggestion": "åŸæ–‡3çš„è¯‘æ–‡å¯èƒ½åœ¨å½“å‰çª—å£ä¹‹åï¼Œå»ºè®®æ‰©å¤§è¯‘æ–‡çª—å£"
  }
}
```

## status è¯´æ˜
- matched: å·²æ‰¾åˆ°å¯¹åº”è¯‘æ–‡
- not_found_maybe_later: æ²¡æ‰¾åˆ°ï¼Œä½†å†…å®¹çœ‹èµ·æ¥åº”è¯¥æœ‰è¯‘æ–‡ï¼Œå¯èƒ½åœ¨åç»­æ®µè½
- not_found_skip: æ²¡æ‰¾åˆ°ï¼Œä½†è¿™ç±»å†…å®¹é€šå¸¸ä¸éœ€è¦ç¿»è¯‘ï¼ˆå¦‚å‡ºç‰ˆä¿¡æ¯ã€é¡µçœ‰é¡µè„šï¼‰
- missing: ç¡®å®šæ˜¯æ¼è¯‘ï¼ˆæ­£æ–‡å†…å®¹ä½†æ²¡æœ‰è¯‘æ–‡ï¼‰

## åˆ¤æ–­"è¯‘æ–‡å¯èƒ½åœ¨åé¢"çš„ä¾æ®
1. åŸæ–‡æ˜¯æ­£æ–‡å†…å®¹ï¼ˆä¸æ˜¯é¡µçœ‰é¡µè„šã€å‡ºç‰ˆä¿¡æ¯ï¼‰
2. åŸæ–‡å†…å®¹å®Œæ•´ï¼Œä¸åƒæ˜¯æ®‹ç¼ºç‰‡æ®µ
3. å½“å‰è¯‘æ–‡çª—å£çš„æœ€åå‡ æ®µè¯‘æ–‡ï¼Œå†…å®¹ä¸Šè¿˜æ²¡æœ‰è¦†ç›–åˆ°è¿™ä¸ªåŸæ–‡
4. é€šå¸¸ç¿»è¯‘æ˜¯æŒ‰é¡ºåºçš„ï¼Œå¦‚æœåé¢çš„åŸæ–‡éƒ½æ‰¾åˆ°äº†è¯‘æ–‡ï¼Œå‰é¢æ²¡æ‰¾åˆ°çš„å¯èƒ½å°±æ˜¯æ¼è¯‘

## åˆ¤æ–­ä¾æ®
1. å†…å®¹è¯­ä¹‰åŒ¹é…ï¼šè¯‘æ–‡æ˜¯å¦è¡¨è¾¾äº†åŸæ–‡çš„æ„æ€
2. æ•°å­—/ä¸“æœ‰åè¯ï¼šåŸæ–‡å’Œè¯‘æ–‡ä¸­çš„æ•°å­—ã€äººåã€åœ°ååº”è¯¥ä¸€è‡´
3. æ®µè½ç»“æ„ï¼šæ®µè½çš„é•¿åº¦å’Œå¤æ‚åº¦åº”è¯¥å¤§è‡´åŒ¹é…
4. é¡ºåºä¸€è‡´æ€§ï¼šé€šå¸¸ç¿»è¯‘ä¼šä¿æŒåŸæ–‡çš„é¡ºåº

è¯·ä»”ç»†åˆ†æä»¥ä¸‹æ–‡æœ¬å¹¶è¾“å‡ºJSONç»“æœï¼š"""

    def __init__(
        self, 
        similarity_threshold: float = 0.25,
        api_key: str = None,
        base_url: str = None,
        alignment_model: str = "x-ai/grok-4.1-fast"
    ):
        """
        Args:
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆç”¨äºè§„åˆ™å¯¹é½ï¼‰
            api_key: API å¯†é’¥ï¼ˆç”¨äºæ™ºèƒ½å¯¹é½ï¼‰
            base_url: API åŸºç¡€ URL
            alignment_model: ç”¨äºå¯¹é½çš„æ¨¡å‹
        """
        self.similarity_threshold = similarity_threshold
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.base_url = base_url or os.getenv("OPENAI_BASE_URL")
        self.alignment_model = alignment_model
        
        # åˆå§‹åŒ– API å®¢æˆ·ç«¯
        if self.api_key:
            client_kwargs = {"api_key": self.api_key}
            if self.base_url:
                client_kwargs["base_url"] = self.base_url
            self.client = OpenAI(**client_kwargs)
        else:
            self.client = None
    
    def smart_align(
        self,
        source_paragraphs: List[Dict],
        target_paragraphs: List[Dict],
        source_window: int = 5,      # åŸæ–‡å°çª—å£
        target_window: int = 30,     # è¯‘æ–‡å¤§çª—å£
        overlap: int = 3,            # é‡å æ®µè½æ•°
        max_retry: int = 2           # æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆæ‰©å¤§çª—å£ï¼‰
    ) -> List[Dict]:
        """
        ä½¿ç”¨æ»‘åŠ¨çª—å£ + å¤§æ¨¡å‹è¿›è¡Œæ™ºèƒ½å¯¹é½ï¼ˆå¸¦å®¹é”™æœºåˆ¶ï¼‰
        
        ç­–ç•¥ï¼š
        - åŸæ–‡ç”¨å°çª—å£ï¼ˆ5æ®µï¼‰ï¼Œè¯‘æ–‡ç”¨å¤§çª—å£ï¼ˆ30æ®µï¼‰
        - æ‰¾åˆ°åŒ¹é…åï¼ŒåŸæ–‡çª—å£æ»‘åŠ¨ï¼Œè¯‘æ–‡çª—å£æ ¹æ®åŒ¹é…ä½ç½®è°ƒæ•´
        - å¦‚æœå¤§æ¨¡å‹åˆ¤æ–­"è¯‘æ–‡å¯èƒ½åœ¨åé¢"ï¼Œä¿ç•™è¿™äº›åŸæ–‡åˆ°ä¸‹ä¸€æ‰¹ï¼Œå¹¶æ‰©å¤§è¯‘æ–‡çª—å£
        - è¾¹ç•Œä¿ç•™é‡å ï¼Œç¡®ä¿è¿ç»­æ€§
        
        Args:
            source_paragraphs: åŸæ–‡æ®µè½åˆ—è¡¨
            target_paragraphs: è¯‘æ–‡æ®µè½åˆ—è¡¨
            source_window: åŸæ–‡çª—å£å¤§å°ï¼ˆé»˜è®¤5ï¼‰
            target_window: è¯‘æ–‡çª—å£å¤§å°ï¼ˆé»˜è®¤30ï¼‰
            overlap: çª—å£é‡å æ®µè½æ•°ï¼ˆé»˜è®¤3ï¼‰
            max_retry: æ‰©å¤§çª—å£çš„æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        if not self.client:
            print("âš ï¸ æœªé…ç½® APIï¼Œä½¿ç”¨è§„åˆ™å¯¹é½")
            return self.align_paragraphs(source_paragraphs, target_paragraphs)
        
        total_sources = len(source_paragraphs)
        total_targets = len(target_paragraphs)
        
        print(f"ğŸ“Š å¼€å§‹æ™ºèƒ½å¯¹é½: {total_sources} ä¸ªåŸæ–‡æ®µè½, {total_targets} ä¸ªè¯‘æ–‡æ®µè½")
        print(f"   çª—å£è®¾ç½®: åŸæ–‡çª—å£={source_window}, è¯‘æ–‡çª—å£={target_window}, é‡å ={overlap}")
        
        # å­˜å‚¨æ‰€æœ‰å¯¹é½ç»“æœ {source_id: {target_ids, confidence, status, note}}
        all_alignments = {}
        
        # éœ€è¦åœ¨ä¸‹ä¸€æ‰¹é‡è¯•çš„åŸæ–‡IDï¼ˆ"å¯èƒ½åœ¨åé¢"çš„ï¼‰
        retry_source_ids = set()
        
        # æ»‘åŠ¨çª—å£ç´¢å¼•
        src_start = 0
        tgt_start = 0
        batch_num = 0
        current_target_window = target_window  # å½“å‰è¯‘æ–‡çª—å£å¤§å°ï¼ˆå¯èƒ½æ‰©å¤§ï¼‰
        
        while src_start < total_sources:
            batch_num += 1
            
            # è·å–å½“å‰åŸæ–‡çª—å£ï¼ˆåŒ…æ‹¬éœ€è¦é‡è¯•çš„ï¼‰
            src_end = min(src_start + source_window, total_sources)
            
            # æ„å»ºæ‰¹æ¬¡åŸæ–‡ï¼šéœ€è¦é‡è¯•çš„ + æ–°çš„åŸæ–‡
            batch_source_ids = []
            batch_sources = []
            
            # å…ˆåŠ å…¥éœ€è¦é‡è¯•çš„åŸæ–‡
            for retry_id in sorted(retry_source_ids):
                if retry_id <= total_sources:
                    batch_source_ids.append(retry_id)
                    batch_sources.append(source_paragraphs[retry_id - 1])
            
            # å†åŠ å…¥æ–°çª—å£çš„åŸæ–‡
            for i in range(src_start, src_end):
                src_id = i + 1
                if src_id not in retry_source_ids:
                    batch_source_ids.append(src_id)
                    batch_sources.append(source_paragraphs[i])
            
            if not batch_sources:
                break
            
            # è·å–å½“å‰è¯‘æ–‡çª—å£ï¼ˆå¯èƒ½æ‰©å¤§ï¼‰
            tgt_end = min(tgt_start + current_target_window, total_targets)
            batch_targets = target_paragraphs[tgt_start:tgt_end]
            
            retry_info = f" (å« {len(retry_source_ids)} ä¸ªé‡è¯•)" if retry_source_ids else ""
            print(f"  æ‰¹æ¬¡ {batch_num}: åŸæ–‡ {batch_source_ids}{retry_info} åœ¨è¯‘æ–‡ [{tgt_start+1}-{tgt_end}] ä¸­æŸ¥æ‰¾")
            
            # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œå¯¹é½
            batch_result = self._align_batch_with_llm(
                batch_sources, 
                batch_targets,
                source_ids=batch_source_ids,  # ä¼ å…¥å®é™…çš„åŸæ–‡ID
                target_offset=tgt_start
            )
            
            # è§£æå¯¹é½ç»“æœ
            s2t_list = batch_result.get("source_to_translation", [])
            window_status = batch_result.get("window_status", {})
            
            # å…¼å®¹æ—§æ ¼å¼
            if not s2t_list:
                t2s_list = batch_result.get("translation_to_source", batch_result.get("alignments", []))
                if t2s_list:
                    s2t_map = {}
                    for t2s in t2s_list:
                        tgt_id = t2s.get("translation_id", 0)
                        for src_id in t2s.get("source_ids", []):
                            if src_id not in s2t_map:
                                s2t_map[src_id] = {
                                    "source_id": src_id,
                                    "translation_ids": [],
                                    "status": "matched",
                                    "confidence": t2s.get("confidence", "medium")
                                }
                            s2t_map[src_id]["translation_ids"].append(tgt_id)
                    s2t_list = list(s2t_map.values())
            
            # æ¸…ç©ºé‡è¯•é›†åˆï¼Œå‡†å¤‡é‡æ–°æ”¶é›†
            retry_source_ids.clear()
            
            # æ”¶é›†æœ¬æ‰¹æ¬¡çš„å¯¹é½ç»“æœ
            max_matched_target = tgt_start  # è®°å½•åŒ¹é…åˆ°çš„æœ€å¤§è¯‘æ–‡ä½ç½®
            need_expand = False
            
            for s2t in s2t_list:
                src_id = s2t.get("source_id", 0)
                tgt_ids = s2t.get("translation_ids", s2t.get("target_ids", []))
                status = s2t.get("status", "matched" if tgt_ids else "missing")
                confidence = s2t.get("confidence", "medium")
                reason = s2t.get("reason", "")
                
                if status == "matched" and tgt_ids:
                    # å·²åŒ¹é…
                    if src_id not in all_alignments:
                        all_alignments[src_id] = {
                            "target_ids": set(tgt_ids),
                            "confidence": confidence,
                            "status": "matched",
                            "note": reason
                        }
                    else:
                        all_alignments[src_id]["target_ids"].update(tgt_ids)
                    
                    # æ›´æ–°æœ€å¤§åŒ¹é…ä½ç½®
                    for tid in tgt_ids:
                        if tid > max_matched_target:
                            max_matched_target = tid
                
                elif status == "not_found_maybe_later":
                    # å¯èƒ½åœ¨åé¢ï¼ŒåŠ å…¥é‡è¯•é›†åˆ
                    retry_source_ids.add(src_id)
                    need_expand = True
                    print(f"    â³ åŸæ–‡{src_id}: å¯èƒ½åœ¨åé¢ - {reason}")
                
                elif status == "not_found_skip":
                    # ä¸éœ€è¦ç¿»è¯‘çš„å†…å®¹ï¼ˆå‡ºç‰ˆä¿¡æ¯ç­‰ï¼‰ï¼Œæ ‡è®°ä¸ºè·³è¿‡
                    all_alignments[src_id] = {
                        "target_ids": set(),
                        "confidence": "high",
                        "status": "skip",
                        "note": reason or "å‡ºç‰ˆä¿¡æ¯/é¡µçœ‰é¡µè„š"
                    }
                    print(f"    â­ï¸ åŸæ–‡{src_id}: è·³è¿‡ - {reason}")
                
                else:  # missing æˆ–å…¶ä»–
                    # ç¡®è®¤æ¼è¯‘
                    all_alignments[src_id] = {
                        "target_ids": set(),
                        "confidence": "low",
                        "status": "missing",
                        "note": reason or "æ¼è¯‘"
                    }
                    print(f"    âš ï¸ åŸæ–‡{src_id}: æ¼è¯‘ - {reason}")
            
            # æ£€æŸ¥ window_status
            if window_status.get("need_expand_window"):
                uncovered = window_status.get("uncovered_sources", [])
                for sid in uncovered:
                    if sid not in all_alignments or all_alignments.get(sid, {}).get("status") != "matched":
                        retry_source_ids.add(sid)
                        need_expand = True
            
            # æ»‘åŠ¨åŸæ–‡çª—å£ï¼ˆè·³è¿‡å·²å¤„ç†å’Œéœ€è¦é‡è¯•çš„ï¼‰
            src_start = src_end
            
            # æ»‘åŠ¨è¯‘æ–‡çª—å£ï¼šæ ¹æ®åŒ¹é…åˆ°çš„æœ€å³ä¾§è¾¹ç•Œ + é‡å 
            # æ–°çš„å·¦è¾¹ç•Œ = æœ€å¤§åŒ¹é…ä½ç½® - overlapï¼ˆä¿ç•™é‡å å®¹é”™ï¼‰
            if max_matched_target > tgt_start:
                # æœ‰æ–°çš„åŒ¹é…ï¼Œæ ¹æ®åŒ¹é…ä½ç½®æ»‘åŠ¨
                new_tgt_start = max(max_matched_target - overlap, tgt_start)
                if new_tgt_start > tgt_start:
                    print(f"    ğŸ“ è¯‘æ–‡çª—å£æ»‘åŠ¨: {tgt_start+1} â†’ {new_tgt_start+1} (æœ€å³åŒ¹é…: è¯‘æ–‡{max_matched_target})")
                    tgt_start = new_tgt_start
            # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼ˆå¯èƒ½æ˜¯å‡ºç‰ˆä¿¡æ¯ç­‰è·³è¿‡çš„éƒ¨åˆ†ï¼‰ï¼Œä¿æŒ tgt_start ä¸å˜
            
            # å¦‚æœéœ€è¦æ‰©å¤§çª—å£ï¼Œå¢åŠ è¯‘æ–‡çª—å£å¤§å°
            if need_expand and retry_source_ids:
                current_target_window = min(target_window * 2, 60)  # æœ€å¤§æ‰©åˆ°60
                print(f"    ğŸ”„ æ‰©å¤§è¯‘æ–‡çª—å£åˆ° {current_target_window}ï¼Œ{len(retry_source_ids)} ä¸ªåŸæ–‡å¾…é‡è¯•")
            else:
                current_target_window = target_window  # æ¢å¤é»˜è®¤
            
            # å¦‚æœè¯‘æ–‡çª—å£å·²ç»æ¥è¿‘æœ«å°¾ä½†è¿˜æœ‰é‡è¯•çš„åŸæ–‡
            if tgt_end >= total_targets and retry_source_ids:
                print(f"  âš ï¸ è¯‘æ–‡å·²åˆ°æœ«å°¾ï¼Œ{len(retry_source_ids)} ä¸ªåŸæ–‡ä»æœªåŒ¹é…ï¼Œæ ‡è®°ä¸ºæ¼è¯‘")
                for sid in retry_source_ids:
                    if sid not in all_alignments:
                        all_alignments[sid] = {
                            "target_ids": set(),
                            "confidence": "low",
                            "status": "missing",
                            "note": "è¯‘æ–‡çª—å£å·²åˆ°æœ«å°¾ä»æœªæ‰¾åˆ°"
                        }
                retry_source_ids.clear()
        
        # å¤„ç†æœ€åå‰©ä½™çš„é‡è¯•åŸæ–‡
        for sid in retry_source_ids:
            if sid not in all_alignments:
                all_alignments[sid] = {
                    "target_ids": set(),
                    "confidence": "low",
                    "status": "missing",
                    "note": "æœ€ç»ˆæœªæ‰¾åˆ°å¯¹åº”è¯‘æ–‡"
                }
        
        print(f"âœ… å¯¹é½å®Œæˆï¼Œå…±å¤„ç† {batch_num} ä¸ªæ‰¹æ¬¡ï¼Œå¾—åˆ° {len(all_alignments)} ä¸ªå¯¹é½ç»“æœ")
        
        # ç»Ÿè®¡
        matched = sum(1 for a in all_alignments.values() if a["status"] == "matched")
        skipped = sum(1 for a in all_alignments.values() if a["status"] == "skip")
        missing = sum(1 for a in all_alignments.values() if a["status"] == "missing")
        print(f"   åŒ¹é…: {matched}, è·³è¿‡: {skipped}, æ¼è¯‘: {missing}")
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        return self._convert_alignments_to_standard(
            all_alignments,
            source_paragraphs,
            target_paragraphs
        )
    
    def _convert_alignments_to_standard(
        self,
        alignments: Dict,  # {source_id: {target_ids, confidence, status, note}}
        source_paragraphs: List[Dict],
        target_paragraphs: List[Dict]
    ) -> List[Dict]:
        """
        å°†å¯¹é½ç»“æœè½¬æ¢ä¸ºæ ‡å‡†è¾“å‡ºæ ¼å¼
        """
        result = []
        
        # éå†æ¯ä¸ªåŸæ–‡æ®µè½
        for src_idx, src_para in enumerate(source_paragraphs):
            src_id = src_idx + 1  # 1-based
            src_text = src_para.get("text", "")
            src_page = src_para.get("page", 1)
            
            align = alignments.get(src_id, {})
            tgt_ids = list(align.get("target_ids", set())) if align else []
            status = align.get("status", "missing") if align else "missing"
            note = align.get("note", "")
            
            if status == "matched" and tgt_ids:
                # æœ‰åŒ¹é…çš„è¯‘æ–‡
                matched_texts = []
                valid_tgt_indices = []
                
                for tid in sorted(tgt_ids):
                    tgt_idx = tid - 1  # 0-based
                    if 0 <= tgt_idx < len(target_paragraphs):
                        matched_texts.append(target_paragraphs[tgt_idx].get("text", ""))
                        valid_tgt_indices.append(tgt_idx)
                
                # åˆå¹¶è¯‘æ–‡
                combined_text = "\n\n".join(matched_texts) if matched_texts else ""
                
                # ç¡®å®š coverage
                coverage = "full"
                if len(valid_tgt_indices) > 1:
                    coverage = "overlap"
                
                result.append({
                    "source_index": src_idx,
                    "target_index": valid_tgt_indices[0] if valid_tgt_indices else None,
                    "target_indices": valid_tgt_indices,
                    "source_text": src_text,
                    "target_text": combined_text,
                    "confidence": {"high": 0.9, "medium": 0.6, "low": 0.3}.get(
                        align.get("confidence", "medium"), 0.6
                    ),
                    "page": src_page,
                    "matched": True,
                    "coverage": coverage,
                    "alignment_note": note,
                    "is_multi_target": len(valid_tgt_indices) > 1
                })
            
            elif status == "skip":
                # ä¸éœ€è¦ç¿»è¯‘çš„å†…å®¹ï¼ˆå‡ºç‰ˆä¿¡æ¯ç­‰ï¼‰
                result.append({
                    "source_index": src_idx,
                    "target_index": None,
                    "target_indices": [],
                    "source_text": src_text,
                    "target_text": None,
                    "confidence": 0.9,  # é«˜ç½®ä¿¡åº¦è·³è¿‡
                    "page": src_page,
                    "matched": False,
                    "coverage": "skip",
                    "alignment_note": note or "å‡ºç‰ˆä¿¡æ¯/é¡µçœ‰é¡µè„šï¼Œæ— éœ€ç¿»è¯‘",
                    "is_multi_target": False
                })
            
            else:
                # æ¼è¯‘æˆ–æœªæ‰¾åˆ°
                result.append({
                    "source_index": src_idx,
                    "target_index": None,
                    "target_indices": [],
                    "source_text": src_text,
                    "target_text": None,
                    "confidence": 0,
                    "page": src_page,
                    "matched": False,
                    "coverage": "missing",
                    "alignment_note": note or "åŸæ–‡ç¼ºå°‘å¯¹åº”è¯‘æ–‡ï¼ˆæ¼è¯‘ï¼‰",
                    "is_multi_target": False
                })
        
        return result
    
    def _align_batch_with_llm(
        self,
        source_batch: List[Dict],
        target_batch: List[Dict],
        source_ids: List[int] = None,  # åŸæ–‡çš„å®é™…IDåˆ—è¡¨
        target_offset: int = 0
    ) -> Dict:
        """
        ä½¿ç”¨å¤§æ¨¡å‹å¯¹é½ä¸€ä¸ªæ‰¹æ¬¡çš„æ®µè½
        
        Args:
            source_batch: åŸæ–‡æ®µè½åˆ—è¡¨
            target_batch: è¯‘æ–‡æ®µè½åˆ—è¡¨
            source_ids: åŸæ–‡çš„å®é™…IDåˆ—è¡¨ï¼ˆæ”¯æŒéè¿ç»­ï¼Œå¦‚ [1,2,5,6]ï¼‰
            target_offset: è¯‘æ–‡çš„åç§»é‡
        """
        # æ„å»ºæç¤º
        prompt = self.ALIGNMENT_PROMPT + "\n\n"
        
        prompt += "## æ³•è¯­åŸæ–‡æ®µè½\n\n"
        for i, para in enumerate(source_batch):
            # ä½¿ç”¨ä¼ å…¥çš„IDæˆ–è®¡ç®—
            src_id = source_ids[i] if source_ids else (i + 1)
            text = para.get("text", "")[:500]  # é™åˆ¶é•¿åº¦
            page = para.get("page", "?")
            prompt += f"[åŸæ–‡{src_id}] (ç¬¬{page}é¡µ)\n{text}\n\n"
        
        prompt += "\n## ä¸­æ–‡è¯‘æ–‡æ®µè½\n\n"
        for i, para in enumerate(target_batch):
            tgt_id = target_offset + i + 1
            text = para.get("text", "")[:500]
            prompt += f"[è¯‘æ–‡{tgt_id}]\n{text}\n\n"
        
        prompt += "\nè¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿° JSON æ ¼å¼è¾“å‡ºå¯¹é½ç»“æœï¼Œç‰¹åˆ«æ³¨æ„åŒºåˆ† status çš„å‡ ç§æƒ…å†µï¼š"
        prompt += "\n- matched: æ‰¾åˆ°å¯¹åº”è¯‘æ–‡"
        prompt += "\n- not_found_maybe_later: æ²¡æ‰¾åˆ°ä½†å¯èƒ½åœ¨åé¢"
        prompt += "\n- not_found_skip: å‡ºç‰ˆä¿¡æ¯ç­‰ä¸éœ€è¦ç¿»è¯‘"
        prompt += "\n- missing: ç¡®è®¤æ¼è¯‘"
        
        try:
            response = self.client.chat.completions.create(
                model=self.alignment_model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=3000
            )
            
            result_text = response.choices[0].message.content
            
            # æå– JSON
            json_match = re.search(r'\{[\s\S]*\}', result_text)
            if json_match:
                result = json.loads(json_match.group())
                # ç¡®ä¿ç»“æœåŒ…å«å¿…è¦å­—æ®µ
                if "source_to_translation" not in result:
                    result["source_to_translation"] = []
                if "window_status" not in result:
                    result["window_status"] = {}
                return result
            else:
                print(f"    è­¦å‘Š: æ— æ³•è§£æå¯¹é½ç»“æœ")
                return {"source_to_translation": [], "window_status": {}}
                
        except Exception as e:
            print(f"    å¯¹é½è°ƒç”¨å¤±è´¥: {e}")
            return {"source_to_translation": [], "window_status": {}}
    
    def align_paragraphs(
        self, 
        source_paragraphs: List[Dict],
        target_paragraphs: List[Dict]
    ) -> List[Dict]:
        """
        åŸºäºè§„åˆ™çš„æ®µè½å¯¹é½ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
        """
        aligned = []
        target_idx = 0
        used_targets = set()
        
        for src_idx, src_para in enumerate(source_paragraphs):
            src_text = src_para.get("text", "")
            src_page = src_para.get("page", 1)
            
            best_match = None
            best_confidence = 0
            best_target_idx = None
            
            # æœç´¢èŒƒå›´
            search_start = max(0, target_idx - 2)
            search_end = min(len(target_paragraphs), target_idx + 8)
            
            for t_idx in range(search_start, search_end):
                if t_idx in used_targets:
                    continue
                    
                tgt_para = target_paragraphs[t_idx]
                tgt_text = tgt_para.get("text", "")
                
                confidence = self._calculate_match_confidence(
                    src_text, tgt_text, 
                    position_diff=abs(t_idx - target_idx)
                )
                
                if confidence > best_confidence:
                    best_confidence = confidence
                    best_target_idx = t_idx
                    best_match = {
                        "target_index": t_idx,
                        "target_text": tgt_text
                    }
            
            if best_match and best_confidence >= self.similarity_threshold:
                aligned.append({
                    "source_index": src_idx,
                    "target_index": best_match["target_index"],
                    "source_text": src_text,
                    "target_text": best_match["target_text"],
                    "confidence": round(best_confidence, 3),
                    "page": src_page,
                    "matched": True
                })
                used_targets.add(best_target_idx)
                target_idx = best_target_idx + 1
            else:
                aligned.append({
                    "source_index": src_idx,
                    "target_index": None,
                    "source_text": src_text,
                    "target_text": None,
                    "confidence": 0,
                    "page": src_page,
                    "matched": False
                })
        
        return aligned
    
    def _calculate_match_confidence(
        self, 
        source: str, 
        target: str,
        position_diff: int = 0
    ) -> float:
        """è®¡ç®—åŒ¹é…ç½®ä¿¡åº¦"""
        if not source or not target:
            return 0.0
        
        scores = []
        weights = []
        
        # é•¿åº¦æ¯”ä¾‹
        src_len = len(source)
        tgt_len = len(target)
        ratio = tgt_len / src_len if src_len > 0 else 0
        
        if 0.25 <= ratio <= 1.0:
            length_score = 1.0 - abs(ratio - 0.55) / 0.45
        elif ratio < 0.25:
            length_score = ratio / 0.25 * 0.5
        else:
            length_score = max(0, 1.0 - (ratio - 1.0) / 2)
        
        scores.append(length_score)
        weights.append(0.35)
        
        # æ•°å­—åŒ¹é…
        src_numbers = set(re.findall(r'\d+', source))
        tgt_numbers = set(re.findall(r'\d+', target))
        if src_numbers:
            common = src_numbers & tgt_numbers
            number_score = len(common) / len(src_numbers)
        else:
            number_score = 1.0
        
        scores.append(number_score)
        weights.append(0.25)
        
        # ä½ç½®è·ç¦»
        position_score = max(0, 1.0 - position_diff * 0.15)
        scores.append(position_score)
        weights.append(0.2)
        
        # ä¸“æœ‰åè¯åŒ¹é…
        src_caps = set(re.findall(r'\b[A-Z][A-Za-z]*\b', source))
        tgt_caps = set(re.findall(r'\b[A-Z][A-Za-z]*\b', target))
        if src_caps:
            caps_common = src_caps & tgt_caps
            caps_score = len(caps_common) / len(src_caps)
        else:
            caps_score = 1.0
        
        scores.append(caps_score)
        weights.append(0.2)
        
        return sum(s * w for s, w in zip(scores, weights))
    
    def calculate_alignment_quality(self, aligned: List[Dict]) -> Dict:
        """è®¡ç®—å¯¹é½è´¨é‡ç»Ÿè®¡"""
        total = len(aligned)
        matched = sum(1 for a in aligned if a.get("matched"))
        avg_confidence = sum(a.get("confidence", 0) for a in aligned) / total if total > 0 else 0
        
        high_conf = sum(1 for a in aligned if a.get("confidence", 0) >= 0.7)
        medium_conf = sum(1 for a in aligned if 0.4 <= a.get("confidence", 0) < 0.7)
        low_conf = sum(1 for a in aligned if 0 < a.get("confidence", 0) < 0.4)
        
        return {
            "total_paragraphs": total,
            "matched_paragraphs": matched,
            "match_rate": matched / total if total > 0 else 0,
            "average_confidence": round(avg_confidence, 3),
            "high_confidence_count": high_conf,
            "medium_confidence_count": medium_conf,
            "low_confidence_count": low_conf,
            "unmatched_count": total - matched
        }


def test_aligner():
    """æµ‹è¯•å¯¹é½åŠŸèƒ½"""
    source_paragraphs = [
        {"text": "Bonjour, comment allez-vous aujourd'hui?", "page": 1},
        {"text": "Il fait beau aujourd'hui. Le soleil brille.", "page": 1},
        {"text": "J'aime lire des livres de philosophie.", "page": 2},
    ]
    
    target_paragraphs = [
        {"text": "ä½ å¥½ï¼Œä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿ", "index": 0},
        {"text": "ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚é˜³å…‰æ˜åªšã€‚", "index": 1},
        {"text": "æˆ‘å–œæ¬¢é˜…è¯»å“²å­¦ä¹¦ç±ã€‚", "index": 2},
    ]
    
    aligner = TextAligner()
    
    # æµ‹è¯•è§„åˆ™å¯¹é½
    print("\nğŸ“Š è§„åˆ™å¯¹é½ç»“æœ:")
    aligned = aligner.align_paragraphs(source_paragraphs, target_paragraphs)
    for item in aligned:
        status = "âœ…" if item["matched"] else "âŒ"
        conf = item["confidence"]
        src_preview = item['source_text'][:30]
        tgt_preview = item['target_text'][:20] if item['target_text'] else 'N/A'
        print(f"{status} [{conf:.2f}] {src_preview}... -> {tgt_preview}...")
    
    quality = aligner.calculate_alignment_quality(aligned)
    print(f"\nğŸ“ˆ å¯¹é½è´¨é‡: åŒ¹é…ç‡ {quality['match_rate']:.1%}")


if __name__ == "__main__":
    test_aligner()
